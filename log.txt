total_epoch = 100 
lr = 0.00005 
transforms.AutoAugment(policy = transforms.AutoAugmentPolicy.CIFAR10, interpolation=transforms.InterpolationMode.BILINEAR)
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.0001)
Finished Training
model saved  ./resnet18_cifar10_95.3300.pth
log_train_acc:  [57.033248081841435, 80.60661764705883, 84.9524456521739, 86.88459079283888, 88.16735933503837, 89.04052109974424, 89.80578644501279, 90.20740089514067, 91.09454923273657, 91.40025575447571, 91.8458280051151, 92.27941176470588, 92.36932544757033, 92.87683823529412, 92.9567615089514, 93.27445652173913, 93.43030690537084, 93.91983695652173, 94.09766624040921, 94.18558184143222, 94.43734015345268, 94.31146099744245, 94.33144181585678, 94.47130754475704, 94.42734974424552, 94.4253516624041, 94.39338235294117, 94.55522698209718, 94.4693094629156, 94.24352621483376, 94.53324808184144, 94.32145140664962, 94.51126918158567, 94.62515984654732, 94.48529411764706, 94.48529411764706, 94.32344948849105, 94.46131713554988, 94.38938618925832, 94.43334398976982, 94.39737851662404, 94.30147058823529, 94.61117327365729, 94.67511189258312, 94.61317135549872, 94.47730179028133, 94.2894820971867, 94.61916560102301, 94.40936700767263, 94.44333439897699, 94.53724424552429, 94.48129795396419, 94.47929987212277, 94.49528452685422, 94.41735933503837, 94.48929028132993, 94.53125, 94.55322890025576, 94.39538043478261, 94.37340153452685, 94.44733056265984, 94.4253516624041, 94.54723465473145, 94.34143222506394, 94.45532289002557, 94.25751278772378, 94.58120204603581, 94.51526534526855, 94.60917519181585, 94.51726342710998, 94.49528452685422, 94.48329603580562, 94.36740728900256, 94.44333439897699, 94.52325767263427, 94.35342071611254, 94.457320971867, 94.50727301790282, 94.50127877237851, 94.5871962915601, 94.55522698209718, 94.44932864450128, 94.45931905370844, 94.4633152173913, 94.457320971867, 94.40936700767263, 94.51126918158567, 94.40337276214834, 94.32145140664962, 94.38139386189258, 94.58120204603581, 94.58919437340154, 94.59718670076727, 94.4253516624041, 94.58120204603581, 94.31345907928389, 94.58320012787723, 94.60118286445012, 94.52125959079284, 94.48329603580562]
log_train_loss:  [1.4413591425894472, 0.6127491773623029, 0.45797490536251945, 0.39112683257940783, 0.35254405459860705, 0.3232820997362399, 0.3026025013050155, 0.28944864427037253, 0.2665733084025438, 0.2538711605760295, 0.24057230557245976, 0.23206657307017642, 0.22715935870395293, 0.2120718310165512, 0.2052473278187425, 0.19501823852615208, 0.19160748524662782, 0.1791896792771795, 0.17602705517950493, 0.17203613779629054, 0.1641510806529952, 0.16724638323849805, 0.16607555311620997, 0.16425478723867204, 0.1632768800482154, 0.165747218443762, 0.1655970471589576, 0.1626189866286638, 0.16446288056490596, 0.17031880526010262, 0.1621343350309469, 0.16652865917243234, 0.16171129788879468, 0.16434422486683214, 0.16383692697452767, 0.16335795317178645, 0.16612262982170067, 0.1647006889514606, 0.16702744066762878, 0.1640782516325831, 0.163975750103288, 0.1682833728149457, 0.1626953032282193, 0.16005934990675705, 0.16157822841849853, 0.16423179935949767, 0.1671099930792056, 0.1614784883964054,
0.16639819977771672, 0.16470430636078195, 0.16389838462252446, 0.16245996087189296, 0.1628217014586529, 0.16445313264494357, 0.1655463518103218, 0.16027966479454048, 0.1642432629804858, 0.16151099731130977, 0.1652951125779649, 0.16626877375804555, 0.16442307798892183, 0.16241195796729277, 0.16474747302158332, 0.16713899327084766, 0.1639244967995359, 0.16815691204774944, 0.16354449730738044, 0.16469818120703217, 0.16095181547886575, 0.1637167917502582, 0.16204503050450322, 0.16591642972777415, 0.16522891815904234, 0.16501186177958652, 0.16326729491676972, 0.1669705293577193, 0.16585548908051934, 0.1642601638151061, 0.16261118524910315, 0.1624060208263719, 0.1632300820535101, 0.16462504290177693, 0.16303655172190856, 0.16565744364943802, 0.16598619341307208, 0.16477388077322633, 0.16386566460227875, 0.165320636730765, 0.16737042826450313, 0.16615325016448337, 0.16312468411577175, 0.16192202179995188, 0.16298678470418201, 0.16411514165322952, 0.16067235842537697, 0.16584149820973043, 0.1613571860296342, 0.16218813810297442, 0.16240933302627958, 0.16204206309879146]
log_test_acc:  [82.27, 89.18, 90.69, 91.71, 92.46, 92.83, 93.35, 93.55, 93.84, 93.86, 94.12, 94.08, 94.42, 94.48, 94.65, 94.62, 94.81, 94.84, 94.92, 95.07, 95.16, 95.07, 95.27, 95.18, 95.32, 95.01, 95.17, 95.16, 95.22, 95.16, 95.0, 95.32, 94.98, 95.19, 95.05, 94.97, 95.08, 95.22, 95.21, 95.18, 95.27, 95.07, 94.98, 95.3, 95.24, 95.09, 95.15, 95.24, 95.16, 95.31, 95.26, 95.24, 95.19, 95.24, 94.91, 95.21, 95.12, 95.08, 95.13, 95.08, 95.02, 95.0, 95.03, 95.04, 95.18, 94.94, 95.05, 95.17, 95.14, 95.17, 95.11, 95.2, 95.15, 95.01, 95.12, 95.2, 95.16, 95.13, 95.3, 95.12, 95.24, 95.04, 95.19, 95.09, 95.31, 95.21, 95.06, 95.26, 95.03, 95.25, 95.18, 95.25, 95.33, 95.24,
95.25, 95.2, 95.15, 95.14, 95.23, 95.02]
log_test_loss:  [0.5532104240568413, 0.3235224062152797, 0.27320447686700916, 0.23761355515021115, 0.21707102334581013, 0.20549304479020103, 0.19385847526572214, 0.18913775925010365, 0.18078207436471763, 0.1752021942193372, 0.17314529952332558, 0.1721506274181955, 0.1658189499834863, 0.16558334990670429, 0.164115483809321, 0.1606191314803649, 0.15546400306064362, 0.154036042503092, 0.15802249953050362, 0.15023885933674425, 0.15084212361685415, 0.15113567241308276, 0.15022690085893156, 0.15072357570971565, 0.14973953127897602, 0.15285702582857152, 0.15099760773686016, 0.15090823607996198, 0.14988330292206853, 0.15117782486777032, 0.15219906203183955, 0.1498147999209384, 0.15230768796512242, 0.14897349272218682, 0.15227598637344944, 0.15143041501016286, 0.15192894188596037, 0.152653684108687, 0.15095501653554086, 0.15166316898151966, 0.14973691081357884, 0.15170109666835913, 0.15379480198057696, 0.14906493191161213, 0.1504054800294045, 0.15219087129913836, 0.15253239754588377, 0.1500844457678591, 0.15004056350855302, 0.15082065585233106, 0.14969308900980344, 0.15019296141910515, 0.15052399701249616, 0.14995070981693195, 0.15324505910431505, 0.15067614652966585, 0.1506576942450234, 0.15137731172498475, 0.15076475140408815, 0.15336681948408903, 0.15331966001046726, 0.1521682948776635, 0.15194820747174076, 0.15257197596819444, 0.15218546708128364, 0.15480093750231586, 0.1534371174669809, 0.15383600044238657, 0.15122112954677783, 0.15053350319800662, 0.1516313875212191, 0.15177410377751013, 0.1510915737953603, 0.1528048187380399, 0.15120361910533697, 0.1520741559235085, 0.15068496758027877, 0.1511953565241062, 0.150929411595958, 0.15196820656744114, 0.14980999133738582, 0.1510689115196743, 0.15072503232801696, 0.15212177399857604, 0.15023091347305667, 0.15147182161427114, 0.1527926392407689, 0.15019957447414367, 0.15184749832652294, 0.1520215903690991, 0.15099497105386891, 0.14994606928526028, 0.15038130615510525, 0.15024783107107187, 0.15086265593131098, 0.1497748681586667,
0.15074243245492874, 0.15168318085379157, 0.15042540447238179, 0.15252281142662721]
                                                                                                                                                                                                                                                                                           Fri Jun 02 23:15              

model_num = 1 
total_epoch = 100 
lr = 0.0001
transforms.AutoAugment(policy = transforms.AutoAugmentPolicy.IMAGENET, interpolation=transforms.InterpolationMode.BILINEAR),
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

Best Accuracy: 96.160000 %
Finished Training
model saved  ./resnet18_cifar10_96.1600.pth
log_train_acc:  [67.23545396419438, 85.1122921994885, 88.24928069053709, 89.77381713554988, 90.96467391304348, 91.84382992327366, 92.40329283887468, 92.88682864450128, 93.60613810741688, 94.0716911764706, 94.57121163682865, 94.87492007672634, 94.91687979539643, 95.24856138107417, 95.35046355498721, 95.83399936061382, 95.81401854219949, 96.00983056265984, 96.05778452685422, 96.26558503836317, 96.4534047314578, 96.6272378516624, 96.74712276214834, 96.6951726342711, 96.90497122762149, 96.90097506393862, 97.06881393861893, 97.14074488491049, 97.11876598465473, 97.14074488491049, 97.15672953964194, 97.21867007672634, 97.40648976982096, 97.39250319693095, 97.46243606138107, 97.41248401534527, 97.37252237851662, 97.39450127877238, 97.5263746803069, 97.58431905370844, 97.57632672634271, 97.54235933503837, 97.4764226342711, 97.52837276214834, 97.62028452685422, 97.69820971867007, 97.64026534526855, 97.71619245524296, 97.76614450127877, 97.64026534526855, 97.67223465473145, 97.8360773657289, 97.75615409207161, 97.79811381074168, 97.7621483375959, 97.69820971867007, 97.73417519181585, 97.75015984654732, 97.71419437340154, 97.7002078005115, 97.81409846547315, 97.79411764705883, 97.82408887468031, 97.88203324808184, 97.76414641943734, 97.74816176470588, 97.7861253196931, 97.76414641943734, 97.82209079283888, 97.80810421994885, 97.8360773657289, 97.75415601023018, 97.75615409207161, 97.81809462915601, 97.77613491048594, 97.88403132992327, 97.85605818414322, 97.730179028133, 97.90001598465473, 97.73617327365729, 97.94597186700767, 97.75415601023018, 97.82408887468031, 97.85406010230179, 97.8600543478261, 97.94597186700767, 97.83208120204604, 97.7861253196931, 97.86804667519182, 97.88602941176471, 97.84406969309462, 97.81010230179028, 97.76614450127877, 97.97594309462916, 97.892023657289, 97.88802749360613, 97.92998721227622, 97.89402173913044, 97.8420716112532, 97.9599584398977]
log_train_loss:  [1.0683920648701661, 0.4451774081115223, 0.35045763279509057, 0.30280792078627344, 0.2658728614468556, 0.24298273193676148, 0.22429536220134066, 0.20868914823531343, 0.18971123103328677, 0.17654702462770444, 0.1595815001793987, 0.15122554339992497, 0.14913901869360063, 0.14021077522260073, 0.13681398147283613, 0.1255156125217114, 0.12385857569725464, 0.11979133175814624, 0.11719362612556466, 0.11022009042537083, 0.10607351943769533, 0.1011719720335701, 0.09872794630544265, 0.09982522196419862, 0.09284111748918739, 0.09299734781217545, 0.09105250431710611, 0.08660873291356598, 0.08770913411470135, 0.08783563415584204, 0.08535093098254803, 0.08425427686370661, 0.0806212005737807, 0.07908094691379411, 0.0773862055121728, 0.07852900133568251, 0.07849107537945003, 0.07816479498248957, 0.07433955360601759, 0.0754831722414102, 0.07440512630042842, 0.07508731619312006, 0.07599874000128148, 0.0729781348455359, 0.07200805028266918, 0.07162987960316718, 0.07139728842285292, 0.07032942564090919, 0.07150199265121614, 0.07144680111478452, 0.0714705829428809, 0.06810690104470724, 0.06920731398026886, 0.06901886711752189, 0.06940119086182378, 0.07035064512847082, 0.06848136508894508, 0.06798327728853468, 0.06840224011058031, 0.07055896968768953, 0.06820979165961333, 0.06833184737523974, 0.0688787437070285, 0.06642026945060152, 0.06842983189477202, 0.06944115267878355, 0.0678209772778918, 0.06823767937274407, 0.06866052324819329, 0.06725299774962085, 0.06870084793558177, 0.06742899243414993, 0.06689251660276442, 0.06864909556351335, 0.06701526506369233, 0.06775349540078579, 0.06590528069528491, 0.0677790206212722, 0.06575572923364123, 0.06923949696740513, 0.06487674820665604, 0.0688792067488222, 0.06721233186857475, 0.06657967817686174, 0.06408744647055674, 0.06457885687925455, 0.06766551966323991, 0.06688448958649107, 0.06740757020647682, 0.06662873748351661, 0.06722054642665645, 0.06688426348828541, 0.06873358559855462, 0.06244527851469586, 0.065623499991377, 0.06602338158889957, 0.06424835507395914, 0.0649953900861418, 0.0676572646968343, 0.06506915003016038]
log_test_acc:  [88.45, 91.41, 92.77, 93.31, 94.12, 94.04, 94.5, 94.75, 94.85, 95.08, 94.9, 95.24, 95.49, 95.32, 95.4, 95.62, 95.36, 95.68, 95.79, 95.76, 95.73, 95.76, 95.74, 95.95, 95.75, 95.95, 95.9, 95.84, 95.89, 95.68, 95.96, 95.8, 95.75, 96.1, 95.77, 95.74, 95.9, 96.07, 95.97, 96.03, 95.99, 95.99, 95.9, 96.03, 95.93, 95.95, 95.85, 96.0, 95.85, 96.05, 96.07, 95.98, 95.95, 96.03, 95.8, 95.95, 95.96, 96.04, 96.16, 96.12, 96.0, 95.99, 96.0, 95.97, 96.06, 96.06, 95.97, 96.1, 96.04, 95.98, 96.0, 95.94, 96.07, 96.0, 95.96, 95.98, 95.99, 96.0, 96.04, 95.95, 95.95, 96.02, 96.06, 95.91, 96.04, 96.01, 95.92, 96.03, 95.92, 96.07, 96.08, 95.95, 96.03, 95.98, 95.99, 95.98, 96.02, 96.01, 95.97, 96.11]
log_test_loss:  [0.37093599186711074, 0.2576111630007928, 0.21574808350112343, 0.19830651027868842, 0.17602733337840284, 0.17058635015878507, 0.15966581212919537, 0.1528364251156283, 0.1523526030110396, 0.14437950561515406, 0.14185956715141998, 0.13924165799719534, 0.13439388523219012, 0.13531721309126238, 0.1346179253877979, 0.12998596616187552, 0.13164493580110193, 0.12810522728915447, 0.12804447043214853, 0.12819185952117465, 0.12787268295627208, 0.12775402951745352, 0.1274183368852408, 0.12519141169695341, 0.12686820780615304, 0.12698615199540553, 0.12649563306010786, 0.1269062544909597, 0.12786222084847887, 0.12968792811708643, 0.12630474011304185, 0.12792537718876545, 0.12670058717555166, 0.12759456595709726, 0.12633651359972314, 0.1277834120869111, 0.1258211943252972, 0.12693504942064887, 0.12759014805744187, 0.12715755247811492, 0.12821297078708757, 0.12649588125801514, 0.13073266994569754, 0.1270122618604895, 0.12511759325283675, 0.12880755629384785, 0.1293554534863717, 0.12568557878141656, 0.12784112131998046, 0.12685410515076062, 0.12514481573955868, 0.1256875237168813, 0.12787124503752215, 0.12583955310898529, 0.12914593062375274, 0.12513702320006317, 0.1271676597030351, 0.12572308322900444, 0.12615165881523252, 0.12610842774065367, 0.12762420995176624, 0.12665757204200545, 0.12669984181845864, 0.12626684110511532, 0.12638333657374182, 0.12913323003491062, 0.12693548474085337, 0.1251914965522912, 0.1260191609410514, 0.12514223499591925, 0.12575138503373154, 0.12731359253270624, 0.1256289228359873, 0.128309402408318, 0.12656735237600386, 0.12698906171635177, 0.12723530550798123, 0.12735857608983472, 0.12592850135101386, 0.12643977951955226, 0.12601379157216153, 0.1265117755197196, 0.12568079008760458, 0.12962544322169833, 0.12657139636410025, 0.12612386368568412, 0.12522198873761198, 0.12503647064950332, 0.12715333400185036, 0.12586590578585038, 0.1271224984154086, 0.12674999644401327, 0.12583002806821986, 0.12517858929193862, 0.12453736981884447, 0.12652092115320368, 0.12623951875406442, 0.12614090592923138, 0.12620877621858032, 0.12512429245924026]
(ych) ych@gnode8:~/cifar-baseline_$ 
  gnode8  0  bash   1  htop                                                                                                                                                                                                                                                                                                                           Sat Jun 03 12:43  

model_num = 1 
total_epoch = 100 
lr = 0.001 
best_acc = 0.0
transforms.AutoAugment(policy = transforms.AutoAugmentPolicy.IMAGENET, interpolation=transforms.InterpolationMode.BILINEAR),
# transforms.RandomHorizontalFlip(),
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
batch_size=256

Finished Training
model saved  ./resnet18_cifar10_96.2000.pth
log_train_acc:  [78.25015943877551, 90.16820790816327, 92.7913743622449, 94.0836256377551, 95.3188775510204, 96.00725446428571, 96.47480867346938, 96.94236288265306, 97.18550701530613, 97.3696588010204, 98.32549426020408, 98.88352997448979, 98.89588647959184, 99.05054209183673, 99.10873724489795, 99.13105867346938, 99.22472895408163, 99.29607780612245, 99.31242028061224, 99.2625956632653, 99.37420280612245, 99.3897480867347, 99.32358099489795, 99.41605548469387, 99.48182397959184, 99.43319515306122, 99.3881536989796, 99.42761479591837, 99.4319993622449, 99.3797831632653, 99.40210459183673, 99.41605548469387, 99.44156568877551, 99.37619579081633, 99.46348852040816, 99.40210459183673, 99.42123724489795, 99.41605548469387, 99.4272161989796, 99.46189413265306, 99.47146045918367, 99.46189413265306, 99.42362882653062, 99.43120216836735, 99.49537627551021, 99.42761479591837, 99.32597257653062, 99.42323022959184, 99.4140625, 99.42163584183673, 99.4515306122449, 99.43797831632654, 99.44595025510205, 99.45352359693878, 99.45591517857143, 99.44196428571429, 99.35626594387755, 99.43598533163265, 99.40011160714286, 99.4076849489796, 99.4351881377551, 99.39572704081633, 99.41964285714286, 99.38018176020408, 99.45192920918367, 99.48939732142857, 99.4515306122449, 99.43000637755102, 99.43160076530613, 99.4399713010204, 99.3602519132653, 99.42004145408163, 99.39811862244898, 99.44595025510205, 99.48062818877551, 99.46747448979592, 99.37420280612245, 99.42004145408163, 99.48142538265306, 99.43319515306122, 99.4678730867347, 99.43000637755102, 99.42402742346938, 99.44794323979592, 99.45591517857143, 99.43120216836735, 99.45990114795919, 99.4678730867347, 99.51729910714286, 99.39811862244898, 99.47943239795919, 99.44595025510205, 99.36822385204081, 99.46707589285714, 99.43399234693878, 99.43757971938776, 99.45990114795919, 99.43598533163265, 99.41206951530613, 99.41206951530613]
log_train_loss:  [0.6580103535433205, 0.285584992976213, 0.2129300449195565, 0.17619359485652983, 0.13876872121983644, 0.11985813963169954, 0.10554164980671235, 0.09129186022114388, 0.08230835282985045, 0.07784325047871288, 0.05107126924760488, 0.0367911104778094, 0.03456754075144702, 0.030679136120277097, 0.02910859180775908, 0.028017605978007218, 0.025279357105170434, 0.02370388051780055, 0.022576976142234494, 0.023739662750776172, 0.021083478192200085, 0.021226559175244932, 0.021849042484156634, 0.019093880559346278, 0.018494443166098197, 0.019384069210250994, 0.020411760606850515, 0.01868976671628806, 0.018383782105140235, 0.020347802404418816, 0.018961844647455275, 0.019787068593990514, 0.018370640056079482, 0.020147624330557123, 0.017924816322712495, 0.0200631080138288, 0.018229323558091204, 0.018121913038621828, 0.018153657486937865, 0.018542533899581402, 0.017898509726736088, 0.017966277516038368, 0.018820706777493184, 0.018305440751683652, 0.016985218378011972, 0.018855465720977863, 0.020755809033289552, 0.018343261135409454, 0.01983176047286513, 0.018843904527246345, 0.018504741001750667, 0.018821436536441347, 0.018518332339709207, 0.01803420198967262, 0.01837316305704453, 0.018483688665211807, 0.020396442282042106, 0.018521208390926143, 0.01850120068114365, 0.01855575338443171, 0.018789279311467722, 0.020328255315615357, 0.019599642072405134, 0.01919374745624254, 0.018580949443573018, 0.017364249782095074, 0.01827125534253689, 0.01905400849099518, 0.018870163857414177, 0.018446866277372465, 0.020392684470291952, 0.019761569931038787, 0.01921914714835204, 0.01845024289724854, 0.018384759225264877, 0.01802750602269507, 0.019389569235499948, 0.018419455234388992, 0.017257540771851734, 0.018532735648403437, 0.017705051357886394, 0.018755829809898778, 0.018813153163396885, 0.018583595431979974, 0.018822152469288176, 0.018265609456017157, 0.018339816413164064, 0.017819741430777903, 0.017608671936168507, 0.01983072058587544, 0.017192868264011885, 0.018316680487668217, 0.0196398960530986, 0.01857736812932987, 0.01866868786318988, 0.018283652068039745, 0.01809009079992467, 0.019225789938473656, 0.019030776984362424, 0.019668384693914607]
log_test_acc:  [90.73, 92.54, 92.84, 93.41, 94.23, 94.31, 94.38, 94.29, 94.49, 94.44, 95.75, 95.95, 95.92, 96.06, 96.06, 95.99, 95.94, 96.05, 96.01, 95.95, 95.96, 95.97, 95.94, 96.03, 96.03, 96.12, 96.07, 96.07, 96.04, 96.2, 96.09, 96.08, 96.07, 96.13, 96.09, 96.12, 96.14, 96.09, 96.11, 96.11, 96.09, 96.07, 96.08, 96.13, 96.04, 96.15, 96.08, 96.09, 96.12, 96.05, 96.1, 96.04, 96.11, 96.1, 96.09, 96.11, 96.1, 96.12, 96.16, 96.17, 96.17, 96.06, 96.11, 96.1, 96.16, 96.18, 96.12, 96.12, 96.05, 96.11, 96.09, 96.08, 96.08, 96.09, 96.09, 96.11, 96.1, 96.11, 96.17, 96.07, 96.05, 96.05, 96.1, 96.07, 96.13, 96.07, 96.05, 96.05, 96.06, 96.09, 96.08, 96.11, 96.07, 96.07, 96.14, 96.16, 96.13, 96.14, 96.03, 96.11]
log_test_loss:  [0.27846297262182884, 0.21731147494846645, 0.20098017883396802, 0.18846407822763153, 0.17682045109830224, 0.17412955605171537, 0.16499008055357764, 0.17479822075519078, 0.18107939603888662, 0.18818436185943754, 0.13553131391432502, 0.1334696255962201, 0.1332693758423231, 0.13575385454814284, 0.13735447068541662, 0.13726412583662786, 0.1354612675182533, 0.13707711479398457, 0.14009184708443012, 0.13863290097633976, 0.1370741979531393, 0.13645814584081778, 0.13559669739566174, 0.13518371232471527, 0.13734509081281504, 0.13556457777882527, 0.13426330412710044, 0.13587426566925656, 0.1369288217028841, 0.1358910452827168, 0.1376672533993322, 0.13690603816159752, 0.13663780751628066, 0.13605282818410966, 0.13691763200308602, 0.1359644821523789, 0.13591383624426323, 0.13692662161918745, 0.13723805550629303, 0.13697366558907118, 0.13722547277709046, 0.13651281367331758, 0.13734129613326285, 0.1371314987978139, 0.13778117274714563, 0.1362200792437632, 0.13686015162347973, 0.1358365906989486, 0.13654466477638472, 0.13756258014740813, 0.13625368179167016, 0.13643119988807997, 0.13723108882183502, 0.13571443148148105, 0.13732356351460595, 0.13631771811255214, 0.13783139098799987, 0.13702618225277757, 0.13666030644236624, 0.13704398820571545, 0.1368886909662484, 0.13641154030505426, 0.13602300976399412, 0.13665969825125376, 0.1369290699966864, 0.13574161387056882, 0.13723567687229274, 0.13702931603742505, 0.13667354977332005, 0.13621655523901352, 0.13637601159086993, 0.1361716370669972, 0.13767001617284172, 0.1368973597040906, 0.13635450730708307, 0.13682043541401884, 0.13742049217238367, 0.13665498911334645, 0.1370062422919112, 0.13652744947600326, 0.13615078931692598, 0.13719227858670835, 0.13587604047150728, 0.1381936614201322, 0.1358577751557543, 0.13677635781610042, 0.13616552340512464, 0.13655439122139268, 0.13775056643493155, 0.1368075584582067, 0.13717447797594692, 0.1362298948667644, 0.13657161834305911, 0.1371089606329855, 0.1368305615129666, 0.13621018232088172, 0.13653375995972, 0.1362637015127125, 0.1366660226207562, 0.1366586839497041]
  
                                                                                                                                                                                                                                                                                                                           Sat Jun 03 12:42  
model_num = 1 
total_epoch = 100 
lr = 0.001
transforms.AutoAugment(policy = transforms.AutoAugmentPolicy.IMAGENET, interpolation=transforms.InterpolationMode.BILINEAR),
# transforms.RandomHorizontalFlip(),
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
batch_size=128

